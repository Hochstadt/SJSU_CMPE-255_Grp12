# -*- coding: utf-8 -*-
"""Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NB1_M-eIUhrn_CfhLWNLO7C4IfrwQ7C5
"""

import os
import urllib.request
from collections import Counter, defaultdict
import numpy as np
from matplotlib import pyplot as plt
from pandas import DataFrame
from sklearn.decomposition import PCA
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler
import zipfile

"""A pretty straightforward way to dynamically load the data per session is to actually just download and unzip the dataset from its source."""

# Download PEMS-SF zip file into temp directory
#!mkdir temp
urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00204/PEMS-SF.zip', 'PEMS-SF.zip')

with zipfile.ZipFile("PEMS-SF.zip","r") as zip_ref:
    zip_ref.extractall("PEMS-SF/")

# Unzip data content to PEMS-SF and remove temp directory
#!unzip -u temp/PEMS-SF.zip -d PEMS-SF
#!rm -r temp

"""The code below reads in the traffic sensor data from the file into a numpy array. 

The input data is formatted such that for each day, there is a matrix starting with `[` and ending with `]\n`. The matrix for any given day has 963 rows, one for each traffic sensor, delimited by `;`. Each row contains 144 space-delimited elements, the 24-hour time series of 10-minute data.

To read in the data, the characters that define the start and end of each day-matrix (`[` and `]\n`) are accounted for by slicing them off each line via `line[1:-2]`. Then, each matrix (at this point a `str`) is then split on `;` to yeild a list of strings, each string representing the space-delimited time series of data for a particular traffic sensor on a particular day. The end result stored in `content` is a list of lists of strings, where each list represents a day, and each element of the day list is a string of space-delimited time series data of a single traffic sensor.

To construct the 2-D data array `X`, the array is initialized to the size of the data set to save processing time (each row is a day, each column is a single data point of a single sensor). The result is a data structure with of size `n` x `m`, where `n` is the number of days in the input file and `m` is the total dimensionality of the dataset (963 sensors * 144 samples per day).
"""

def read_file(train_test='train', return_meta_data=False):
    """Read input data file and parse into 2-D numpy array"""
    print('Reading input data...')
    with open(r'PEMS-SF/PEMS_'+train_test) as f:
        content = [line[1:-2].split(';') for line in list(f)]
    
    num_days = len(content)
    num_sensors = len(content[0])
    pnts_per_day = len(content[0][0].split())

    print('Number of days:', num_days)  # should be 267 for train, 173 for test
    print('Number of sensors:', num_sensors)  # should be 963
    print('Data points per day:', pnts_per_day)  # should be 144
    
    print('Processing into 2-D data array...')
    X = np.zeros((num_days,num_sensors*pnts_per_day))
    for i, day in enumerate(content):
        for j, sensor in enumerate(day):
            X[i,j*pnts_per_day:(j+1)*pnts_per_day] = np.array(sensor.split(), dtype=float)

    if return_meta_data == True:
        return (X, (num_days, num_sensors, pnts_per_day))
    else:
        return X
  
def read_file_3d(train_test='train'):
    """Read input data file and parse into 3-D numpy array"""
    print('Reading input data...')
    with open(r'PEMS-SF/PEMS_'+train_test) as f:
        content = [line[1:-2].split(';') for line in list(f)]
    
    num_days = len(content)
    num_sensors = len(content[0])
    pnts_per_day = len(content[0][0].split())

    print('Number of days:', num_days)  # should be 267 for train, 173 for test
    print('Number of sensors:', num_sensors)  # should be 963
    print('Data points per day:', pnts_per_day)  # should be 144
    
    print('Processing into 3-D data array...')
    X = np.zeros((num_days,num_sensors,pnts_per_day))
    for i, day in enumerate(content):
        for j, sensor in enumerate(day):
            X[i,j] = np.array(sensor.split(), dtype=float)

    return X

def read_labels(train_test='train'):
    with open(r'PEMS-SF/PEMS_'+train_test+'labels') as f:
        content = [int(x) for x in f.read()[1:-2].split(' ')]
    return content

def convert_3D_to_2D(data, mode='3D_2D_Standard'):
    """Awkward function but just wanted some easy array conversions"""
    if mode == '3D_2D_Standard':  # Convert from 3D (day, sensor, point_idx) to the original 2D array (day, sensor data) (sensor[0]pt[0] sensor[0]pt[1] ...)
        return data.transpose([0,1,2]).reshape(data.shape[0], data.shape[1]*data.shape[2])
    elif mode == '3D_2D_Sensors': # Convert from 3D (day, sensor, point_idx) to 2D array: (sensor, point in time) (day[0]pt[0] day[0]pt[1] ...)
        return data.transpose([1,0,2]).reshape(data.shape[1], data.shape[0]*data.shape[2])
    else:
        raise ValueError(f'Function does not support mode {mode}.')
        return None

def convert_2D_to_3D(data, n_days, n_sensors, points_per_day, mode='2D_3D_Standard'):
    """Second awkward function, 2D to 3D matrix"""
    X = np.zeros((n_days,n_sensors,points_per_day))
    if mode == '2D_3D_Standard': # Convert from 2D array (day, sensor data) (sensor[0]pt[0] sensor[0]pt[1] ...) to 3D (day, sensor, point_idx):
        for i, day in enumerate(data):
            for j in range(n_sensors):
                X[i][j] = day[j*points_per_day:(j+1)*points_per_day]
        return X
    elif mode == '2D_3D_Sensors': # Convert from 2D array: (sensor, point in time) (day[0]pt[0] day[0]pt[1] ...) to 3D (day, sensor, point_idx)
        for j, sensor in enumerate(data):
            for i in range(n_days):
                X[i][j] = sensor[i*(points_per_day):(i+1)*(points_per_day)]
        return X
    else:
        raise ValueError(f'Function does not support mode {mode}.')
        return None

(X, (num_days, num_sensors, pnts_per_day)) = read_file(return_meta_data=True)
labels = read_labels()
test_X = read_file('test')
test_labels = read_labels('test')
print('Data has', X.shape[0], 'rows and', X.shape[1], 'columns.')
print('Test data has', test_X.shape[0], 'rows and', test_X.shape[1], 'columns.')

train_3D = read_file_3d()
test_3D  = read_file_3d('test')

num_days     = train_3D.shape[0]
num_sensors  = train_3D.shape[1]
pnts_per_day = train_3D.shape[2]

num_days_test     = test_3D.shape[0]
num_sensors_test  = test_3D.shape[1] # We already know this will be the same as the training set
pnts_per_day_test = test_3D.shape[2] # We already know this will be the same as the training set

print('Data has', num_days, 'days and', num_sensors, 'sensors, with', pnts_per_day, 'data points for each sensor for any single day')
print('Test data has', num_days_test, 'days and', num_sensors_test, 'sensors, with', pnts_per_day_test, 'data points for each sensor for any single day')

DAY    = 35
SENSOR = 41
TIME   = 95 # 3:50 PM = 15 hrs + 50 minutes past midnight => 15*6 + 5 = 95
print(f'Day {DAY}, Sensor #{SENSOR}, {TIME*10} minutes past midnight: {train_3D[DAY][SENSOR][TIME]} occupancy rate')

# Bi-directional dict just to keep track a little easier
DAYS_str_to_label = {
    'Sunday': 1,
    'Monday': 2,
    'Tuesday': 3,
    'Wednesday': 4,
    'Thursday': 5 ,
    'Friday': 6,
    'Saturday': 7
}
DAYS_label_to_str = {
    1: 'Sunday',
    2: 'Monday',
    3: 'Tuesday' ,
    4: 'Wednesday',
    5: 'Thursday',
    6: 'Friday',
    7: 'Saturday'
}

def boxplot_day(input, day_labels, day_to_plot, dt=6, average_flatten='flatten', show_outliers=False, dbg=False, show_averages=False):
    """
    Create a figure of boxplots, averaging the sensor data for each time increment in a given day
    day = 1-7, Sunday - Saturday
    dt             = number of datapoints per boxplot, default of 6 = 60 minutes
    average_flatten = 
    """
    indices = [i for i in range(len(day_labels)) if day_labels[i] == day_to_plot]
    data_per_day = input[indices]

    num_increments = int(data_per_day.shape[2] / dt)

    # Agnostic of sensor, just general data per timepoint
    new_time = {}
    once = True
    for i in range(num_increments):
        if average_flatten == 'flatten':
            new_time[i] = data_per_day[:,:,dt*i:dt*(i+1)].flatten()
        elif average_flatten == 'average':
            all_data = list()
            for inc in data_per_day[:,:,dt*i:dt*(i+1)]:
                for cur in inc:
                    all_data.append(np.mean(cur))
            if dbg == True:
                new_time[i] = np.array([np.mean(cur) for cur in inc for inc in data_per_day[:,:,dt*i:dt*(i+1)]])
            elif dbg == False:
                new_time[i] = np.array(all_data)
        else:
            new_time[i] = data_per_day[:,:,dt*i:dt*(i+1)].flatten()

    fig, ax = plt.subplots(figsize=(15,7))
    plot = ax.boxplot(new_time.values(), showfliers=show_outliers)
    fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,
                    hspace=0.4, wspace=0.3)
    fig.suptitle(DAYS_label_to_str[day_to_plot], fontsize=20)
    ax.set_xticklabels([x for x in range(num_increments)])
    if not show_outliers:
        plt.ylim([0, 0.25])
    plt.xlabel('Hour of the Day')
    plt.ylabel('Average Occupancy Rate')

    if show_averages:
        for i, line in enumerate(plot['medians']):
            x, y = line.get_xydata()[1]
            text = ' Î¼={:.2f}'.format(new_time[i].mean())
            ax.annotate(text, xy=(x, -0.02))

def plot_bad_histogram(input, day_labels, day_to_plot, dt=6, idx=0, show_outliers=False, step=True):
    indices = [i for i in range(len(day_labels)) if day_labels[i] == day_to_plot]
    data_per_day = input[indices]

    num_increments = int(data_per_day.shape[2] / dt)

    # Agnostic of sensor, just general data per timepoint
    new_time = {}
    for i in range(num_increments):
        new_time[i] = data_per_day[:,:,dt*i:dt*(i+1)].flatten()
    x = new_time[idx]

    if show_outliers == False:
        df = DataFrame(x, columns=['thedata'])
        Q1 = df['thedata'].quantile(0.25)
        Q3 = df['thedata'].quantile(0.75)
        IQR = Q3 - Q1

        indices = (df['thedata'] >= Q1 - 1.5 * IQR) & (df['thedata'] <= Q3 + 1.5 *IQR)
        x = df[indices].to_numpy()

    q25, q75 = np.percentile(x, [.25, .75])
    bin_width = 2*(q75 - q25)*len(x)**(-1/3)
    bins = round((x.max() - x.min())/bin_width)

    fig, ax = plt.subplots(figsize=(18,10))
    if step == True:
        plt.hist(x, bins=int(bins), histtype='step')
    else:
        plt.hist(x, bins=bins)
    fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,
                    hspace=0.4, wspace=0.3)
    fig.suptitle(DAYS_label_to_str[day_to_plot], fontsize=20)
    plt.xlabel('Occupancy Rate')
    plt.ylabel('Occurances')

def plot_single_sensor_data(input, sensor_idx=0):
    x = input[:,sensor_idx:,:].flatten()

    q25, q75 = np.percentile(x, [.25, .75])
    bin_width = 2*(q75 - q25)*len(x)**(-1/3)
    bins = round((x.max() - x.min())/bin_width)

    fig, ax = plt.subplots(figsize=(5,5))
    plt.hist(x, bins=int(bins), histtype='step')
    fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,
                   hspace=0.4, wspace=0.3)
    fig.suptitle(f'Sensor {sensor_idx}', fontsize=20)
    plt.xlabel('Occupancy Rate')
    plt.ylabel('Occurances')

def plot_all_sensor_data(input, day_labels, num_sensors):

    def ceildiv(a, b):
        return -(-a // b)

    num_cols = 10
    num_rows = ceildiv(num_sensors, num_cols)
    print(f'num_cols: {num_cols}, num_rows:  {num_rows}')
    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30,num_rows*7), )
    axs = axs.ravel()
    for i, ax in enumerate(axs):
        if i >= num_sensors:
            break
        x = input[:,i,:].flatten()

        # Looks like bin width can end up being too small
        #q25, q75 = np.percentile(x, [.25, .75])
        #print(f'q25: {q25}, q75: {q75}')
        #bin_width = 2*(q75 - q25)*len(x)**(-1/3)
        #print(f'bin_width: {bin_width}')
        #bins = round((x.max() - x.min())/bin_width)

        #ax.hist(x, bins=bins, histtype='step')
        ax.hist(x, histtype='step')
        ax.set_title(f'Sensor {i}')

# for day in DAYS_label_to_str:
#     boxplot_day(train_3D, labels, day)
plot_bad_histogram(train_3D, labels, DAYS_str_to_label['Sunday'], step=True)
boxplot_day(train_3D, labels, DAYS_str_to_label['Sunday'], average_flatten='flatten', show_outliers=False)
boxplot_day(train_3D, labels, DAYS_str_to_label['Sunday'], average_flatten='average', show_outliers=False)
boxplot_day(train_3D, labels, DAYS_str_to_label['Tuesday'], average_flatten='flatten', show_outliers=False)
boxplot_day(train_3D, labels, DAYS_str_to_label['Tuesday'], average_flatten='average', show_outliers=False)
boxplot_day(train_3D, labels, DAYS_str_to_label['Sunday'], average_flatten='flatten', show_outliers=True)
boxplot_day(train_3D, labels, DAYS_str_to_label['Sunday'], average_flatten='average', show_outliers=True)
boxplot_day(train_3D, labels, DAYS_str_to_label['Tuesday'], average_flatten='flatten', show_outliers=True)
boxplot_day(train_3D, labels, DAYS_str_to_label['Tuesday'], average_flatten='average', show_outliers=True)
plot_single_sensor_data(train_3D, sensor_idx=777)
#plot_all_sensor_data(train_3D, labels, num_sensors)

# Sensor matrix to use PCA to reduce sensors that don't offer useful variance
sensors_train = convert_3D_to_2D(train_3D, mode='3D_2D_Sensors')
sensors_test  = convert_3D_to_2D(test_3D, mode='3D_2D_Sensors')

# Standard scaler
sensors_train_std = StandardScaler().fit_transform(sensors_train)
sensors_test_std  = StandardScaler().fit_transform(sensors_test)

PCA_VARIANCE = 0.90

s_PCA = PCA()
s_PCA.fit(sensors_train_std.T) # Expects format (n_samples, n_features)

# Find the number of components needed to capture X % of the variance
num_components = 0
for i, current_sum in enumerate(s_PCA.explained_variance_ratio_.cumsum()):
    if current_sum >= PCA_VARIANCE:
        num_components = i
        break
print(f'Number of components (Sensors) needed to capture {PCA_VARIANCE * 100}% of the variance: {num_components}')
# New number of sensors after applying PCA
num_sensors_PCA = num_components

print(f'sensors_train_std.shape before PCA:           {sensors_train_std.shape}')
s_PCA = PCA(n_components=num_components)
sensors_train_PCAd = s_PCA.fit_transform(sensors_train_std.T).T
print(f'sensors_train_PCAd.shape after PCA:          {sensors_train_PCAd.shape}')

# Now we have data by sensor reduced by PCA. need to apply the transformation to the test set.
# So to do that we need to transform the test set into same format above
#sensors_test = convert_3D_to_2D(test_3D, mode='3D_2D_Sensors') 
print(f'Test dimensions before PCA transform: {sensors_test_std.shape}')
sensors_test_PCAd = s_PCA.transform(sensors_test_std.T).T
print(f'Test dimensions after PCA transform:  {sensors_test_PCAd.shape}')

# need to convert back to the form (day, sensor-data)
#                      sensor[0] sensor[1] ... sensor[n]

train_PCAd = convert_3D_to_2D(convert_2D_to_3D(sensors_train_PCAd, num_days, num_sensors_PCA, pnts_per_day, mode='2D_3D_Sensors'), mode='3D_2D_Standard')
test_PCAd  = convert_3D_to_2D(convert_2D_to_3D(sensors_test_PCAd, num_days_test, num_sensors_PCA, pnts_per_day, mode='2D_3D_Sensors'), mode='3D_2D_Standard')

# Guassian Naive Bayes performance testing
print('Guassian Naive Bayes classifer performance testing')
gaussian = GaussianNB()
model = gaussian.fit(X, labels)
predicted_labels = model.predict(test_X)
print(f'Accuracy with original {num_sensors} sensors:')
print(f'\t{sum(1 for x,y in zip(test_labels, predicted_labels) if x == y) * 100 / len(test_labels):.2f}%')

gaussian = GaussianNB()
model = gaussian.fit(train_PCAd, labels)
predicted_labels = model.predict(test_PCAd)
print(f'Accuracy after applying PCA to reduce to {num_sensors_PCA} sensors:')
print(f'\t{sum(1 for x,y in zip(test_labels, predicted_labels) if x == y) * 100 / len(test_labels):.2f}%')

def kNN_classify(test_vect, train, labels, k):
    """Borrowed heavily from our kNN activity in class"""
    dots = test_vect.dot(train.T)  # using dot product as distance metric
    sims = list(zip(labels, dots))

    if len(sims) == 0:
        # could not find any neighbors, return random day
        return np.random.randint(1, 8)

    sims.sort(key=lambda x: x[1], reverse=True)
    tallies = Counter(s[0] for s in sims[:k])
    majority = tallies.most_common(2)

    # majority is list of tuples, each tuple is (int label, int count)
    if len(majority) < 2 or majority[0][1] > majority[1][1]:  
        # majority vote
        return majority[0][0]
    
    # tie break, only get here if the vote is tied
    majority = defaultdict(float)
    for label, similarity in sims[:k]:
        majority[label] += similarity  # summing up the similarities
    #return class w/ highest summed similarity
    return sorted(majority.items(), key=lambda x: x[1], reverse=True)[0][0]  
    
# normalize the pre- and post-PCA datasets and run through kNN 
# need to normalize because kNN uses dot product as distance metric
X_norm = normalize(X)
test_X_norm = normalize(test_X)
train_PCAd_norm = normalize(train_PCAd)
test_PCAd_norm = normalize(test_PCAd)

accuracys = []
for k in range(1, 11):
    classifications = [kNN_classify(x, X_norm, labels, k) for x in test_X_norm]
    acc = 0.0
    for i in range(len(classifications)):
        if test_labels[i] == classifications[i]:
            acc += 1
    acc /= len(classifications)
    accuracys.append(acc)
    print(f'Accuracy for k={k} pre-PCA: {acc*100:.2f}%')
plt.figure()
plt.plot(range(1,11), accuracys, label='pre-PCA')
accuracys = []
for k in range(1, 11):
    classifications = [kNN_classify(x, train_PCAd_norm, labels, k) for x in test_PCAd_norm]
    acc = 0.0
    for i in range(len(classifications)):
        if test_labels[i] == classifications[i]:
            acc += 1
    acc /= len(classifications)
    accuracys.append(acc)
    print(f'Accuracy for k={k} post-PCA: {acc*100:.2f}%')
plt.plot(range(1,11), accuracys, label='post-PCA')
plt.legend()
plt.xlabel('k-value')
plt.ylabel('Accuracy')
plt.title('kNN Classication')

# Perceptron performance testing
print('Perceptron classifer performance testing')
model = Perceptron()
model.fit(X, labels)
predicted_labels = model.predict(test_X)
print(f'Accuracy with original {num_sensors} sensors:')
print(f'\t{sum(1 for x,y in zip(test_labels, predicted_labels) if x == y) * 100 / len(test_labels):.2f}%')

model = Perceptron()
model.fit(train_PCAd, labels)
predicted_labels = model.predict(test_PCAd)
print(f'Accuracy after applying PCA to reduce to {num_sensors_PCA} sensors:')
print(f'\t{sum(1 for x,y in zip(test_labels, predicted_labels) if x == y) * 100 / len(test_labels):.2f}%')

class Layer():
    """Layer object represents a single layer of a neural network.

    num_nodes - number of nodes in this layer, a.k.a. width
    prev_layer - Layer object of previous layer feeding into this one
    function - string name of activation function (ReLU, sigmoid, sign, linear)
    rate - float learning rate, a.k.a. lambda
    """
    def __init__(self, num_nodes, prev_layer=[], function='sigmoid', rate=0.1):
        self.num_nodes = num_nodes
        self.prev_layer = prev_layer
        self.function = function
        self.rate = rate
        
        self.outputs = np.zeros((num_nodes, 1))
        self.gradients = np.zeros((num_nodes, 1))
        self.values = np.zeros((num_nodes, 1))
        self.biases = np.zeros((num_nodes, 1))
        
        # Turns out, how you initialize the weights is important. This was a popular choice
        if not prev_layer:
            self.weights = np.array([])
        else:
            self.weights = np.random.randn(num_nodes, prev_layer.num_nodes) * np.sqrt(2/prev_layer.num_nodes)
    
    
    def feedforward(self, inputs=None):
        """Use vals for the input layer (must be column vector numpy array)."""
        # Update values either by prescribed input or by calculating from previous layer
        if inputs is not None:
            if len(inputs) != self.num_nodes:
                raise ValueError(f'Number of values must be number of nodes in layer ({len(inputs)} != {self.num_nodes})')
            if inputs.shape[1] != 1:
                raise ValueError(f'Must be column vector, input was of shape {inputs.shape} (rows, columns).')
            self.values = inputs
        else:
            self.values = self.weights.dot(self.prev_layer.outputs) + self.biases
        
        # Update outputs by whatever activation function was chosen
        if self.function == 'sigmoid':
            self.outputs = 1 / (1 + np.exp(-1*self.values))

        elif self.function == 'sign':
            for i, val in enumerate(self.values):
                if val >= 0:
                    self.outputs[i] = 1
                else:
                    self.outputs[i] = -1
        
        elif self.function == 'ReLU':
            self.outputs = np.maximum(self.values, 0)
        
        elif self.function == 'linear':
            self.outputs = np.copy(self.values)
    
    
    def backpropagate(self, errors):
        """truths must be column vector numpy array."""
        if len(errors) != self.num_nodes:
            raise ValueError(f'Number of values must be number of nodes in layer ({len(errors)} != {self.num_nodes})')
        if errors.shape[1] != 1:
            raise ValueError(f'Must be column vector, input was of shape {errors.shape} (rows, columns).')
        
        # Update gradients by whatever activation function was chosen
        if self.function == 'sigmoid':
            gradients = self.outputs * (1 - self.outputs)  #S'(x)=S(x)*(1-S(x))
        
        elif self.function in ('sign', 'linear'):
            gradients = np.ones((self.num_nodes, 1))
        
        elif self.function == 'ReLU':
            gradients = np.zeros((self.num_nodes, 1))
            for i, val in enumerate(self.outputs):  # need a sanity check that this shouldn't be self.outputs
                if val > 0:
                    gradients[i] = 1
        
        # Calculate the errors in the output of the previous layer
        prev_layer_errors = np.dot(self.weights.T, errors)
        
        delta_bias = self.rate * errors * gradients  # scaler multiplication and element-wise column vector multiplication -> vector
        delta_weights = np.dot(delta_bias, self.prev_layer.outputs.T)  # not 100% sure about dimensionality here but should end up as matrix
        
        self.biases += delta_bias
        self.weights += delta_weights

        return prev_layer_errors


class Network():
    """Neural network object.

    in_width - integer number of nodes in input layer
    out_width - integer number of nodes in output layer, less than in_width
    depth - integer number of layers in the network, minimum 2
    width - definable with shape='square', number of nodes in each hidden layer
    function - string name of activation function (ReLU, sigmoid, sign, linear)
    rate - float learning rate, a.k.a. lambda
    """
    def __init__(self, in_width, out_width, depth, hidden_width=10, function='sigmoid', rate=0.1):
        
        # Create input layer with proper number of nodes
        self.layers = [Layer(in_width, function=function, rate=rate)]  # input layer
        
        # Create hidden layers (if depth > 2), defaulting to 10 nodes each
        for d in range(depth-2):
            hidden_layer = Layer(hidden_width, self.layers[-1], function=function, rate=rate)
            self.layers.append(hidden_layer)

        # Create output layer
        output_layer = Layer(out_width, self.layers[-1], function=function, rate=rate)
        self.layers.append(output_layer)
        
        # Default empty output
        self.outputs = np.array([])

    def predict(self, inputs):
        """inputs must be column vector numpy array."""
        if len(inputs) != self.layers[0].num_nodes:
            raise ValueError(f'Number of values must be number of nodes in input layer ({len(inputs)} != {self.layers[0].num_nodes})')
        if inputs.shape[1] != 1:
            raise ValueError(f'Must be column vector, input was of shape {inputs.shape} (rows, columns).')
        
        # Set up input
        self.layers[0].feedforward(inputs)
        
        # Propagate signals through the network
        for layer in self.layers[1:]:
            layer.feedforward()
        
        # Get result
        self.outputs = self.layers[-1].outputs
    
    def train(self, truths):
        """truths must be column vector numpy array."""
        # Divide layers for clarity, don't need to backprop to input
        hidden_layers = self.layers[1:-1]
        output_layer = self.layers[-1]
        
        # Make sure the input aligns with expectations
        if len(truths) != output_layer.num_nodes:
            raise ValueError(f'Number of values must be number of nodes in output layer ({len(truths)} != {output_layer.num_nodes})')
        if truths.shape[1] != 1:
            raise ValueError(f'Must be column vector, input was of shape {truths.shape} (rows, columns).')
        
        # Find initial error and the error of the previous layer
        errors = truths - self.outputs
        prev_layer_errors = output_layer.backpropagate(errors)
        
        # Backproppagate the error through the network, updating weights/biases as we go
        for hidden_layer in hidden_layers[::-1]:
            prev_layer_errors = hidden_layer.backpropagate(prev_layer_errors)
        

# Fix this tomorrow and run with the XOR training (randomized)
def train_network(network, train_X, train_Y, train_limit=1000):
    # Initialize
    epoch = 0
    indicies = np.arange(len(train_X))
    
    # Train until we converge on a set of weights (currently only works for 2-layers, input & output)
    while epoch < train_limit:
        
        # Randomize the input for training
        np.random.shuffle(indicies)
        
        # Loop through each data point in the training set, train based on truth (Stocastic Gradient Descent)
        for i in indicies:   
            
            # Set up input as column vector and feedforward through the network
            input_ = train_X[i].reshape(-1, 1)
            network.predict(input_)

            # Get result at final output layer (optional)
            outputs = network.outputs
            
            # Ensure truths is properly formatted
            output_layer = network.layers[-1]
            if output_layer.num_nodes == 1:
                truths = np.array([train_Y[i]], ndmin=2)
            else:
                truths = np.array(train_Y[i], ndmin=2).T
            
            # Backpropagate error through the network based on truth
            network.train(truths)
        
        # Increment and print status if another 10% complete
        epoch += 1
        #if epoch % (train_limit / 10) == 0:
            #print(f'Completed training epoch {epoch}')
        
    #print('Training Complete!')

"""The cell below runs through the XOR example."""

# Training XOR
x = np.array([
    [0,0],
    [0,1],
    [1,0],
    [1,1]
    ])

y = np.array([0,1,1,0])

network = Network(2, 1, 3, hidden_width=2, function='sigmoid', rate=0.5)
train_network(network, x, y, train_limit=100000)

# Let's see how the newly trained network does with each input value now.
for i, input_ in enumerate(x):
    input_ = input_.reshape(-1, 1)
    network.predict(input_)
    print(f'Output: {network.outputs}\tShould Be: {y[i]}')

network = Network(train_PCAd.shape[1], 7, 5, function='sigmoid', rate=0.001)

# make labels into arrays where a 1 at an index indicates that the label is index+1
Y = np.array([np.zeros(7)]*len(labels))
for i, val in enumerate(labels):
    Y[i][val-1] = 1

#train_network(network, X_norm, Y, train_limit=20)
try_num = 1
f = open(f'training_set_{try_num}.csv','w+')

training_results = {try_num: []}
epochs = 1
while True:
    train_network(network, train_PCAd, Y, train_limit=1)
    classifications = []
    for input_ in test_PCAd:
        input_ = input_.reshape(-1, 1)
        network.predict(input_)
        outputs = network.outputs
        index = np.argmax(outputs)
        try:
            # Value is an iterable, meaning there was multiple maximums...
            index = index[0]
        except IndexError:
            # Value is not an iterable, meaning there was only one maximum, which is what we want!
            pass
        index += 1
        classifications.append(index)

    acc = 0.0
    for i in range(len(classifications)):
        if test_labels[i] == classifications[i]:
            acc += 1
    acc /= len(classifications)
    print(f'Accuracy for {epochs} epochs: {acc*100:.2f}%')
    f.write(f'{epochs},{acc}\n')
    training_results[try_num].append((epochs, acc))
    epochs += 1
    if acc < 0.12:
        print('It broke, starting over.')
        f.close()
        try_num += 1
        f = open(f'training_set_{try_num}.csv','w+')
        training_results[try_num] = []
        # try again, see if the gods of luck smile upon you
        network = Network(train_PCAd.shape[1], 7, 5, function='sigmoid', rate=0.001)
        epochs = 1
    if try_num > 5:
        break
